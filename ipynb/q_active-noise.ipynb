{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d990d3-cb59-4d4c-b905-17328c02e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "sys.path.append('/your_path/QIAL')\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from layer.GENERAL_RAM import GENERAL_RAM\n",
    "\n",
    "from utils.info import *\n",
    "\n",
    "from easyfsl.samplers import TaskSampler\n",
    "from easyfsl.utils import plot_images, sliding_average\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from itertools import product\n",
    "from utils.dataManager import *\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage import io\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.metrics import accuracy_score, normalized_mutual_info_score\n",
    "import time\n",
    "import numpy as np\n",
    "import threading\n",
    "\n",
    "from skimage import io, img_as_float\n",
    "\n",
    "import concurrent.futures\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5082e9-77cc-4b9e-8dc1-f253bb51604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = set_params()\n",
    "\n",
    "args = parser.parse_args()\n",
    "args = parser.parse_args(args=['--model', 'GENERAL_RAM',\n",
    "                               '--seed','500',\n",
    "                               '--strategy','ALL',\n",
    "                               '--data','FashionMNIST',                    \n",
    "                               '--N_WAY','4',\n",
    "                               '--N_TRAIN','100',\n",
    "                               '--N_VALIDATE','20',\n",
    "                               '--N_TEST','100',\n",
    "                               '--N_SHOT','30',\n",
    "                               '--N_ACTIVE','30',\n",
    "                               '--N_ACTIVE_TIMES','30',\n",
    "                               '--classes','0','1','2','3',\n",
    "                               '--num_layers','4',\n",
    "                               '--num_qubits','8',\n",
    "                               '--learning_rate','1e-1'])\n",
    "\n",
    "\n",
    "\n",
    "setup_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff2e01-e3fb-4e9c-9188-9c274dc5c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = sample_data_qac(args)\n",
    "\n",
    "images_train = torch.cat([data.reshape(1,-1) for data in images['train']], dim=0)\n",
    "images_valid = torch.cat([data.reshape(1,-1) for data in images['validate']], dim=0)\n",
    "images_test = torch.cat([data.reshape(1,-1) for data in images['test']], dim=0)\n",
    "\n",
    "labels_train = torch.stack(labels['train'])\n",
    "labels_valid = torch.stack(labels['validate'])\n",
    "labels_test = torch.stack(labels['test'])\n",
    "\n",
    "plot_images(images['train'], \"query images\", images_per_row=args.N_TRAIN)\n",
    "\n",
    "X_train = torch.tensor(normalize(torch.tensor(images_train.view(args.N_TRAIN*args.N_WAY, -1), requires_grad=False),norm='l2'))\n",
    "label_train = torch.tensor(labels_train, requires_grad=False)\n",
    "Y_train= torch.tensor(torch.zeros([label_train.shape[0], args.N_WAY]), requires_grad=False)\n",
    "for i in range(label_train.shape[0]):\n",
    "    Y_train[i, int(label_train[i])]=1  \n",
    "\n",
    "X_test =  torch.tensor(normalize(torch.tensor(images_test.view(args.N_TEST*args.N_WAY, -1), requires_grad=False),norm='l2'))\n",
    "label_test = torch.tensor(labels_test, requires_grad=False)\n",
    "Y_test= torch.tensor(torch.zeros([label_test.shape[0], args.N_WAY]), requires_grad=False)\n",
    "for i in range(label_test.shape[0]):\n",
    "    Y_test[i, int(label_test[i])]=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d141f-8bdf-4dfd-a3d6-308dded5868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSIM_compute(images, args):\n",
    "    N = len(images['train'])\n",
    "    \n",
    "    mat = torch.Tensor(N,N)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            img1 = img_as_float(images['train'][i]).reshape(args.N_SIZE,args.N_SIZE)\n",
    "            img2 = img_as_float(images['train'][j]).reshape(args.N_SIZE,args.N_SIZE)\n",
    "            ssim_value, _ = ssim(img1, img2, multichannel=False, full=True)\n",
    "            mat[i, j] = ssim_value.item()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "AdjMtx = SSIM_compute(images, args)\n",
    "topo_list = torch.sum(AdjMtx > 0.7, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcafa7db-bfe0-434d-8cd2-c5bbc9e2e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_loss(predictions_FSL, Y_FSL):\n",
    "    diff = predictions_FSL - Y_FSL\n",
    "    error = torch.trace(torch.matmul(diff.T, diff))\n",
    "    return error\n",
    "\n",
    "def semi_loss_qcnn(predictions_FSL, predictions_CL1, predictions_CL2, Y_FSL, alpha):\n",
    "    contrative_loss = CLoss_select('Distance')(predictions_CL1, predictions_CL2)\n",
    "    diff = 1.0 - torch.sum(predictions_FSL[:, 0]==YSL)/ Y_FSL.shape[0]\n",
    "    error = torch.trace(torch.matmul(diff.T, diff))\n",
    "    loss = alpha*contrative_loss + (1-alpha) * error\n",
    "    return loss\n",
    "\n",
    "def cost(qnet, weights, X_FSL_FRQI, Y_FSL):\n",
    "    pred = qnet(weights, X_FSL_FRQI)\n",
    "    loss = semi_loss(qnet(weights, X_FSL_FRQI), Y_FSL)\n",
    "    return qnet(weights, X_FSL_FRQI), loss\n",
    "    \n",
    "def active_learning(weights_init, args, quantum_neural_network, state_fsl, y_fsl, state_query, y_query, X_test_state, Y_test, AdjMtx):\n",
    "    logger = get_logger(str(args.strategy)+'_'+str(args.data) + '_' + str(args.data_method) + '_' + str(args.N_SHOT) + '_' + str(args.num_layers) + '.log')\n",
    "    accs = []\n",
    "    weight_history = []\n",
    "    weight_history.append(weights_init)\n",
    "    \n",
    "    for it_ac in range(args.N_ACTIVE_TIMES+1):\n",
    "        weights = weights_init\n",
    "        opt = torch.optim.Adam([weights], lr = args.learning_rate)\n",
    "        for it in range(100):\n",
    "\n",
    "            opt.zero_grad()\n",
    "            train_res, loss = cost(quantum_neural_network.qnode_qnn, weights, state_fsl, y_fsl)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_acc = accuracy(train_res, y_fsl)\n",
    "\n",
    "        state_fsl, y_fsl, state_query, y_query, AdjMtx = add_sample(quantum_neural_network, weights, state_fsl, y_fsl, state_query, y_query, AdjMtx, args)\n",
    "        \n",
    "        res = quantum_neural_network.qnode_qnn(weights, X_test_state) \n",
    "        test_acc = accuracy(res, Y_test)\n",
    "        \n",
    "        logger.info('{}\\t Epoch_AC:[{}/{}]\\t TRAIN ACCURACY={:.6f}\\t  TEST ACCURACY={:.6f}'.format(args.strategy, it_ac, args.N_ACTIVE_TIMES, train_acc, test_acc))\n",
    "        \n",
    "        print(torch.sum(y_fsl, dim=0))\n",
    "        accs.append(test_acc)\n",
    "        weight_history.append(weights)\n",
    "    \n",
    "    return accs, weight_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29b987-6d36-4088-ba87-d7d8f64f139e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6e0779-fe46-46a8-b855-7716b42cc3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zeros = 0.0 * torch.randn(X_train.shape[1], requires_grad=False)\n",
    "noises = 0.0 * torch.randn(X_train.shape[1], requires_grad=False)\n",
    "\n",
    "if args.model=='GENERAL_RAM':\n",
    "    weights_init = torch.normal(mean=0.0, std=1, size=(args.num_layers,args.num_qubits,3),requires_grad=True)\n",
    "    quantum_neural_network = GENERAL_RAM(args.num_qubits)\n",
    "    \n",
    "X_train_state = []\n",
    "X_test_state = []\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train_state.append(quantum_neural_network.qnode_amplitude(X_train[i,:], zeros))\n",
    "\n",
    "X_train_state = torch.stack(X_train_state)\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test_state.append(quantum_neural_network.qnode_amplitude(X_test[i,:], zeros))\n",
    "    \n",
    "X_test_state = torch.stack(X_test_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9ceae-2895-41e3-a64f-b35a0f0c48ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_distance(s1,s2):\n",
    "    tr_distances = []\n",
    "    n_rows = s1.shape[0]\n",
    "    \n",
    "    for i in range(n_rows):\n",
    "        rho1 = np.outer (np.conjugate(s1[i,:]), s1[i,:])\n",
    "        rho2 = np.outer (np.conjugate(s2[i,:]), s2[i,:])\n",
    "        difference = rho1 - rho2\n",
    "        eigenvalues = np.linalg.eigvals(difference)\n",
    "        tr_distance = np.sum(np.abs(eigenvalues)) / 2\n",
    "        tr_distances.append(tr_distance)\n",
    "    \n",
    "    return tr_distances\n",
    "    \n",
    "\n",
    "dic_dists = {}\n",
    "dic_accs = {}\n",
    "dic_X_train_state = {}\n",
    "dic_weights = {}\n",
    "\n",
    "noise_weight = [0, 0.005, 0.01, 0.015, 0.02]\n",
    "for w in noise_weight:\n",
    "    noise = w * torch.randn(X_train.shape[1], requires_grad=False)\n",
    "    X_train_state_noise = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        state_noise= quantum_neural_network.qnode_amplitude(X_train[i,:]+noise, zeros)\n",
    "        X_train_state_noise.append(state_noise)\n",
    "\n",
    "    X_train_state_noise = torch.stack(X_train_state_noise)\n",
    "    \n",
    "    dists =  trace_distance(X_train_state, X_train_state_noise)\n",
    "    dic_dists[w] = sum(dists)/len(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb211a-8d8f-4d09-a771-b2aa0160b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "categories = list(dic_dists.keys())\n",
    "values = list(dic_dists.values())\n",
    "x_labels = range(len(values))\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'black', 'orange']\n",
    "\n",
    "bar_width = 0.003\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.bar(categories, values, width=bar_width, alpha=0.6, color=colors)\n",
    "\n",
    "\n",
    "for i in range(len(values)):\n",
    "    plt.text(x=categories[i], y=values[i]+0.005, s=f\"{values[i]:.4f}\", ha='center')\n",
    "\n",
    "plt.axhline(y=1/math.e, color='r', linewidth=1, linestyle='--')\n",
    "\n",
    "plt.title(args.data)\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('Trace Distance')\n",
    "\n",
    "plt.savefig(r'../Figure/QIAL_'+'all_'+str(args.N_ACTIVE)+'-'+str(args.seed)+'-'+str(args.num_layers)+'_noise_bar.pdf', format='pdf', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff698f-7256-426c-afaf-c2141a0faaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = np.array([it for it in range(args.N_ACTIVE_TIMES+1)] )\n",
    "\n",
    "y_0 = dic_accs[noise_weight[0]]\n",
    "y_1 = dic_accs[noise_weight[1]]\n",
    "y_2 = dic_accs[noise_weight[2]]\n",
    "y_3 = dic_accs[noise_weight[3]]\n",
    "y_4 = dic_accs[noise_weight[4]]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "line1 = ax.plot(nums, y_0, marker='o', linestyle='-', color='red', \n",
    "                markersize=2, linewidth=2, label='0.0')\n",
    "line2 = ax.plot(nums, y_1, marker='o', linestyle='-', color='blue', \n",
    "                markersize=2, linewidth=2, label='0.005')\n",
    "line3 = ax.plot(nums, y_2, marker='o', linestyle='-', color='green', \n",
    "                markersize=2, linewidth=2, label='0.01')\n",
    "line4 = ax.plot(nums, y_3, marker='o', linestyle='-', color='black', \n",
    "                markersize=2, linewidth=2, label='0.015')\n",
    "line5 = ax.plot(nums, y_4, marker='o', linestyle='-', color='orange', \n",
    "                markersize=2, linewidth=2, label='0.02')\n",
    "\n",
    "ax.set_ylabel('Classification ACC ($\\%$)', fontsize=16)\n",
    "ax.set_xlabel('Active learning rounds', fontsize=16)\n",
    "ax.set_title(args.data)\n",
    "\n",
    "ax.set_ylim([0.6, 1.0])\n",
    "\n",
    "legend = ax.legend()\n",
    "\n",
    "plt.savefig(r'../Figure/QIAL_'+'all_'+str(args.N_ACTIVE)+'-'+str(args.seed)+'-'+str(args.num_layers)+'_noise.pdf', format='pdf', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f86f19c-8b37-438d-a7cf-b5e134e9771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {\n",
    "    'dists_list':dic_dists,\n",
    "    'weight_history_quantum': dic_weights,\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'X_train_state': X_train_state,\n",
    "    'X_test_state': X_test_state,\n",
    "    'Y_train': Y_train,\n",
    "    'Y_test': Y_test,\n",
    "    'accs_quantum': dic_accs,\n",
    "    'args':args\n",
    "} \n",
    "\n",
    "\n",
    "file_path = (\n",
    "    '../results/' + args.data + '/' +\n",
    "    args.data + '_all' + '_'+str(args.seed)+'_noise.pkl'\n",
    ")\n",
    "\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(variables, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Quantum",
   "language": "python",
   "name": "quantum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
